import torch
import numpy as np
import scipy.stats as stats


class LpLoss:
    """Relative/absolute Lp norm loss for comparing fields on uniform meshes."""

    def __init__(self, d=2, p=2, size_average=True, reduction=True):
        assert d > 0 and p > 0
        self.d = d
        self.p = p
        self.reduction = reduction
        self.size_average = size_average

    def abs(self, x, y):
        num_examples = x.size()[0]
        h = 1.0 / (x.size()[1] - 1.0)
        all_norms = (h ** (self.d / self.p)) * torch.norm(
            x.view(num_examples, -1) - y.view(num_examples, -1), self.p, 1
        )
        if self.reduction:
            return torch.mean(all_norms) if self.size_average else torch.sum(all_norms)
        return all_norms

    def rel(self, x, y):
        num_examples = x.size()[0]
        diff_norms = torch.norm(
            x.reshape(num_examples, -1) - y.reshape(num_examples, -1), self.p, 1
        )
        y_norms = torch.norm(y.reshape(num_examples, -1), self.p, 1)
        if self.reduction:
            return torch.mean(diff_norms / y_norms) if self.size_average else torch.sum(diff_norms / y_norms)
        return diff_norms / y_norms

    def __call__(self, x, y):
        return self.rel(x, y)


class HsLoss:
    """Sobolev/HS norm: compares derivatives in Fourier space.

    H^k norm = sum_{|alpha|<=k} a[alpha] * || D^alpha (x - y) ||_p
    computed via: weight = sqrt(1 + a0^2*(kx^2+ky^2) + a1^2*(kx^4+...))
    """

    def __init__(self, d=2, p=2, k=1, a=None, group=False,
                 size_average=True, reduction=True):
        assert d > 0 and p > 0
        self.d = d
        self.p = p
        self.k = k
        self.balanced = group
        self.reduction = reduction
        self.size_average = size_average
        if a is None:
            a = [1] * k
        self.a = a

    def rel(self, x, y):
        num_examples = x.size()[0]
        diff_norms = torch.norm(
            x.reshape(num_examples, -1) - y.reshape(num_examples, -1), self.p, 1
        )
        y_norms = torch.norm(y.reshape(num_examples, -1), self.p, 1)
        if self.reduction:
            return torch.mean(diff_norms / y_norms) if self.size_average else torch.sum(diff_norms / y_norms)
        return diff_norms / y_norms

    def __call__(self, x, y, a=None):
        nx = x.size()[1]
        ny = x.size()[2]
        k = self.k
        a = self.a
        x = x.view(x.shape[0], nx, ny, -1)
        y = y.view(y.shape[0], nx, ny, -1)

        k_x = torch.cat((
            torch.arange(start=0, end=nx // 2, step=1),
            torch.arange(start=-nx // 2, end=0, step=1),
        ), 0).reshape(nx, 1).repeat(1, ny)
        k_y = torch.cat((
            torch.arange(start=0, end=ny // 2, step=1),
            torch.arange(start=-ny // 2, end=0, step=1),
        ), 0).reshape(1, ny).repeat(nx, 1)
        k_x = torch.abs(k_x).reshape(1, nx, ny, 1).to(x.device)
        k_y = torch.abs(k_y).reshape(1, nx, ny, 1).to(x.device)

        x = torch.fft.fftn(x, dim=[1, 2])
        y = torch.fft.fftn(y, dim=[1, 2])

        if not self.balanced:
            weight = 1
            if k >= 1:
                weight = weight + a[0] ** 2 * (k_x ** 2 + k_y ** 2)
            if k >= 2:
                weight = weight + a[1] ** 2 * (k_x ** 4 + 2 * k_x ** 2 * k_y ** 2 + k_y ** 4)
            weight = torch.sqrt(weight)
            loss = self.rel(x * weight, y * weight)
        else:
            loss = self.rel(x, y)
            if k >= 1:
                weight = a[0] * torch.sqrt(k_x ** 2 + k_y ** 2)
                loss += self.rel(x * weight, y * weight)
            if k >= 2:
                weight = a[1] * torch.sqrt(k_x ** 4 + 2 * k_x ** 2 * k_y ** 2 + k_y ** 4)
                loss += self.rel(x * weight, y * weight)
            loss = loss / (k + 1)
        return loss


def relative_l2_error(pred, target):
    """Mean relative L2 error: mean(||pred - target||_2 / ||target||_2)."""
    return torch.mean(
        torch.linalg.norm(pred - target, dim=1)
        / torch.linalg.norm(target, dim=1)
    )


def rmse(pred_mean, target, y_std=None):
    """Root mean squared error, optionally scaled by y_std."""
    val = torch.sqrt(torch.mean((pred_mean - target) ** 2))
    if y_std is not None:
        val = y_std * val
    return val.detach()


def nlpd_marginal(pred_dist, y_test, y_std=1.0):
    """Negative log predictive density using marginal Gaussians."""
    y_np = y_test.cpu().numpy()
    means = pred_dist.loc.detach().cpu().numpy()
    var = pred_dist.covariance_matrix.diag().detach().cpu().numpy()
    log_std = np.log(y_std) if isinstance(y_std, (int, float)) else np.log(y_std.cpu().numpy())
    lpds = []
    for i in range(len(y_np)):
        pp_lpd = stats.norm.logpdf(y_np[i], loc=means[i], scale=np.sqrt(var[i])) - log_std
        lpds.append(np.mean(pp_lpd))
    return -np.mean(lpds)
